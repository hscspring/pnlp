<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [åŠŸèƒ½ç‰¹æ€§](#%E5%8A%9F%E8%83%BD%E7%89%B9%E6%80%A7)
- [å®‰è£…](#%E5%AE%89%E8%A3%85)
- [ä½¿ç”¨](#%E4%BD%BF%E7%94%A8)
  - [æ–‡æœ¬IO](#%E6%96%87%E6%9C%ACio)
    - [IO å¤„ç†](#io-%E5%A4%84%E7%90%86)
    - [å†…ç½®æ–¹æ³•](#%E5%86%85%E7%BD%AE%E6%96%B9%E6%B3%95)
  - [æ–‡æœ¬å¤„ç†](#%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86)
    - [æ¸…ç†å’Œæå–](#%E6%B8%85%E7%90%86%E5%92%8C%E6%8F%90%E5%8F%96)
    - [å†…ç½®æ­£åˆ™](#%E5%86%85%E7%BD%AE%E6%AD%A3%E5%88%99)
  - [æ–‡æœ¬åˆ‡åˆ†](#%E6%96%87%E6%9C%AC%E5%88%87%E5%88%86)
    - [ä»»æ„éƒ¨åˆ†åˆ‡åˆ†](#%E4%BB%BB%E6%84%8F%E9%83%A8%E5%88%86%E5%88%87%E5%88%86)
    - [åˆ†å¥](#%E5%88%86%E5%8F%A5)
    - [ä¸­æ–‡å­—ç¬¦åˆ‡åˆ†](#%E4%B8%AD%E6%96%87%E5%AD%97%E7%AC%A6%E5%88%87%E5%88%86)
    - [å¥å­åˆ†ç»„](#%E5%8F%A5%E5%AD%90%E5%88%86%E7%BB%84)
  - [æ–‡æœ¬å¢å¼º](#%E6%96%87%E6%9C%AC%E5%A2%9E%E5%BC%BA)
    - [Tokençº§åˆ«](#token%E7%BA%A7%E5%88%AB)
    - [å¥å­çº§åˆ«](#%E5%8F%A5%E5%AD%90%E7%BA%A7%E5%88%AB)
  - [æ–‡æœ¬å½’ä¸€åŒ–](#%E6%96%87%E6%9C%AC%E5%BD%92%E4%B8%80%E5%8C%96)
    - [ä¸­æ–‡æ•°å­—](#%E4%B8%AD%E6%96%87%E6%95%B0%E5%AD%97)
  - [æ ¼å¼è½¬æ¢](#%E6%A0%BC%E5%BC%8F%E8%BD%AC%E6%8D%A2)
    - [BIOè½¬å®ä½“](#bio%E8%BD%AC%E5%AE%9E%E4%BD%93)
  - [å†…ç½®è¯å…¸](#%E5%86%85%E7%BD%AE%E8%AF%8D%E5%85%B8)
    - [åœç”¨è¯](#%E5%81%9C%E7%94%A8%E8%AF%8D)
  - [æ–‡æœ¬é•¿åº¦](#%E6%96%87%E6%9C%AC%E9%95%BF%E5%BA%A6)
  - [é­”æœ¯æ–¹æ³•](#%E9%AD%94%E6%9C%AF%E6%96%B9%E6%B3%95)
  - [å¹¶è¡Œå¤„ç†](#%E5%B9%B6%E8%A1%8C%E5%A4%84%E7%90%86)
- [æµ‹è¯•](#%E6%B5%8B%E8%AF%95)
- [æ›´æ–°æ—¥å¿—](#%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

NLP é¢„/åå¤„ç†å·¥å…·ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ä¸“ä¸ºæ–‡æœ¬ IO è®¾è®¡çš„çµæ´»çš„ Pipeline
- çµæ´»çš„æ–‡æœ¬æ¸…ç†/æå–å·¥å…·
- æ–‡æœ¬å¢å¼º
- æŒ‰å¥åˆ‡åˆ†æˆ–æŒ‰ä¸­æ–‡å­—ç¬¦åˆ‡åˆ†æ–‡æœ¬
- æ–‡æœ¬åˆ†æ¡¶
- ä¸­æ–‡å­—ç¬¦å½’ä¸€åŒ–
- æ–‡æœ¬å„ç§é•¿åº¦è®¡ç®—
- ä¸­è‹±æ–‡å¸¸ç”¨åœç”¨è¯
- é¢„å¤„ç†é­”æœ¯æ–¹æ³•
- å¹¶å‘ã€æ‰¹é‡åŒ–ã€å®ä½“ BIO è½¬å®ä½“

## å®‰è£…

éœ€è¦ Python3.7+ã€‚

`pip install pnlp`

## ä½¿ç”¨

### æ–‡æœ¬IO

#### IO å¤„ç†

```bash
tree tests/piop_data/
â”œâ”€â”€ a.md
â”œâ”€â”€ b.txt
â”œâ”€â”€ c.data
â”œâ”€â”€ first
â”‚Â Â  â”œâ”€â”€ fa.md
â”‚Â Â  â”œâ”€â”€ fb.txt
â”‚Â Â  â”œâ”€â”€ fc.data
â”‚Â Â  â””â”€â”€ second
â”‚Â Â      â”œâ”€â”€ sa.md
â”‚Â Â      â”œâ”€â”€ sb.txt
â”‚Â Â      â””â”€â”€ sc.data
â”œâ”€â”€ json.json
â”œâ”€â”€ outfile.file
â”œâ”€â”€ outjson.json
â””â”€â”€ yml.yml
```

```python
import os
from pnlp import Reader

DATA_PATH = "./pnlp/tests/piop_data/"
pattern = '*.md' # å¯ä»¥æ˜¯ '*.txt', 'f*.*' ç­‰ï¼Œæ”¯æŒæ­£åˆ™
reader = Reader(pattern, use_regex=True)

# è·å–æ‰€æœ‰æ–‡ä»¶çš„è¡Œï¼Œè¾“å‡ºè¡Œæ–‡æœ¬ã€è¡Œç´¢å¼•å’Œæ‰€åœ¨çš„æ–‡ä»¶å
for line in reader(DATA_FOLDER_PATH):
    print(line.lid, line.fname, line.text)
"""
0 a.md line 1 in a.
1 a.md line 2 in a.
2 a.md line 3 in a.
0 fa.md line 1 in fa.
1 fa.md line 2 in fa
...
"""

# è·å–æŸä¸ªæ–‡ä»¶çš„æ‰€æœ‰è¡Œï¼Œè¾“å‡ºè¡Œæ–‡æœ¬ã€è¡Œç´¢å¼•å’Œæ‰€åœ¨æ–‡ä»¶åï¼Œæ­¤æ—¶ç”±äºæŒ‡å®šäº†æ–‡ä»¶å pattern æ— æ•ˆ
for line in reader(os.path.join(DATA_FOLDER_PATH, "a.md")):
    print(line.lid, line.fname, line.text)
"""
0 a.md line 1 in a.
1 a.md line 2 in a.
2 a.md line 3 in a.
"""



# è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„
for path in Reader.gen_files(DATA_PATH, pattern, use_regex: True):
    print(path)
"""
pnlp/tests/piop_data/a.md
pnlp/tests/piop_data/first/fa.md
pnlp/tests/piop_data/first/second/sa.md
"""

# è·å–ä¸€ä¸ªç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶åå’Œå®ƒä»¬çš„å†…å®¹
paths = Reader.gen_files(DATA_PATH, pattern)
articles = Reader.gen_articles(paths)
for article in articles:
    print(article.fname)
    print(article.f.read())
"""
a.md
line 1 in a.
line 2 in a.
line 3 in a.
...
"""

# åŒå‰ä¸¤ä¸ªä¾‹å­
paths = Reader.gen_files(DATA_PATH, pattern)
articles = Reader.gen_articles(paths)
for line in Reader.gen_flines(articles, strip="\n"):
    print(line.lid, line.fname, line.text)
```

#### å†…ç½®æ–¹æ³•

```python
import pnlp

# Read
file_string = pnlp.read_file(file_path)
file_list = pnlp.read_lines(file_path)
file_json = pnlp.read_json(file_path)
file_yaml = pnlp.read_yaml(file_path)
file_csv = pnlp.read_csv(file_path)
file_pickle = pnlp.read_pickle(file_path)
list_dict = pnlp.read_file_to_list_dict(file_path)

# Write
pnlp.write_json(file_path, data, indent=2)
pnlp.write_file(file_path, data)
pnlp.write_pickle(file_path, data)
pnlp.write_list_dict_to_file(file_path, data)

# Others
pnlp.check_dir(dirname) # å¦‚æœç›®å½•ä¸å­˜åœ¨ä¼šåˆ›å»º
```

### æ–‡æœ¬å¤„ç†

#### æ¸…ç†å’Œæå–

```python
import re
from pnlp import Text

text = "è¿™æ˜¯https://www.yam.gifté•¿åº¦æµ‹è¯•ï¼Œã€Š ã€‹*)FSJfdsjfğŸ˜![](http://xx.jpg)ã€‚233."
pattern = re.compile(r'\d+')

# pattern æ˜¯ re.Pattern ç±»å‹æˆ– str ç±»å‹
# é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ï¼š'', è¡¨ç¤ºä¸ä½¿ç”¨ä»»ä½• patternï¼ˆå®é™…æ˜¯ re.compile(r'.+')ï¼‰ï¼Œæ­¤æ—¶ clean è¿”å›ç©ºï¼ˆå…¨éƒ¨è¢«æ¸…äº†ï¼‰ï¼Œextract è¿”å›åŸå§‹æ–‡æœ¬ã€‚
# pattern æ”¯æŒä»¥ä¸‹å­—ç¬¦ä¸²ç±»å‹ï¼ˆå®é™…ä¸ºæ­£åˆ™ï¼‰ï¼š
#	'chi': ä¸­æ–‡å­—ç¬¦
#	'pun': æ ‡ç‚¹
#	'whi': ç©ºç™½
#	'nwh': éç©ºç™½
#	'wnb': å­—æ¯ï¼ˆå«ä¸­æ–‡å­—ç¬¦ï¼‰æˆ–æ•°å­—
#	'nwn': éå­—æ¯ï¼ˆå«ä¸­æ–‡å­—ç¬¦ï¼‰æˆ–æ•°å­—
#	'eng': è‹±æ–‡å­—ç¬¦
#	'num': æ•°å­—
#	'pic': å›¾ç‰‡
#	'lnk': é“¾æ¥
#	'emj': è¡¨æƒ…

pt = Text(['chi', pattern])

# æå–æ‰€æœ‰ç¬¦åˆ pattern çš„æ–‡æœ¬å’Œå®ƒä»¬çš„ä½ç½®
res = pt.extract(text)
print(res)
"""
{'text': 'è¿™æ˜¯é•¿åº¦æµ‹è¯•233', 'mats': ['è¿™æ˜¯', 'é•¿åº¦æµ‹è¯•', '233'], 'locs': [(0, 2), (22, 26), (60, 63)]}
"""
# æ”¯æŒç”¨ã€Œç‚¹ã€è·å–keyå±æ€§
print(res.text, res.mats, res.locs)
"""
'è¿™æ˜¯é•¿åº¦æµ‹è¯•' ['è¿™æ˜¯', 'é•¿åº¦æµ‹è¯•'] [(0, 2), (22, 26)]
"""

# è¿”å›æŒ‡å®š pattern æ¸…ç†åçš„æ–‡æœ¬
print(pt.clean(text))
"""
https://www.yam.giftï¼Œã€Š ã€‹*)FSJfdsjfğŸ˜![](http://xx.jpg)ã€‚233.
"""

# å¯ä»¥æŒ‡å®šå¤šä¸ª patternï¼Œæ³¨æ„å…ˆåé¡ºåºå¯èƒ½ä¼šå½±å“ç»“æœå“¦
pt = Text(['pic', 'lnk'])
# æå–åˆ°çš„
res = pt.extract(text)
print(res.mats)
"""
['https://www.yam.gif',
 '![](http://xx.jpg)',
 'https://www.yam.gift',
 'http://xx.jpg']
"""
# æ¸…ç†åçš„
print(pt.clean(text))
"""
è¿™æ˜¯té•¿åº¦æµ‹è¯•ï¼Œã€Š ã€‹*)FSJfdsjfğŸ˜ã€‚233.
"""
```

#### å†…ç½®æ­£åˆ™

```python
# USE Regex
from pnlp import reg
def clean_text(text: str) -> str:
    text = reg.pwhi.sub("", text)
    text = reg.pemj.sub("", text)
    text = reg.ppic.sub("", text)
    text = reg.plnk.sub("", text)
    return text
```

### æ–‡æœ¬åˆ‡åˆ†

#### ä»»æ„éƒ¨åˆ†åˆ‡åˆ†

```python
# Cut by Regex
from pnlp import cut_part, psent
text = "ä½ å¥½ï¼æ¬¢è¿ä½¿ç”¨ã€‚"
sent_list = cut_part(text, psent, with_spliter=True, with_offset=False)
print(sent_list)
"""
['ä½ å¥½ï¼', 'æ¬¢è¿ä½¿ç”¨ã€‚']
"""
pcustom_sent = re.compile(r'[ã€‚ï¼]')
sent_list = cut_part(text, pcustom_sent, with_spliter=False, with_offset=False)
print(sent_list)
"""
['ä½ å¥½', 'æ¬¢è¿ä½¿ç”¨']
"""
sent_list = cut_part(text, pcustom_sent, with_spliter=False, with_offset=True)
print(sent_list)
"""
[('ä½ å¥½', 0, 3), ('æ¬¢è¿ä½¿ç”¨', 3, 8)]
"""
```

#### åˆ†å¥

```python
# Cut Sentence
from pnlp import cut_sentence as pcs
text = "ä½ å¥½ï¼æ¬¢è¿ä½¿ç”¨ã€‚"
sent_list = pcs(text)
print(sent_list)
"""
['ä½ å¥½ï¼', 'æ¬¢è¿ä½¿ç”¨ã€‚']
"""
```

#### ä¸­æ–‡å­—ç¬¦åˆ‡åˆ†

```python
# ä¸­æ–‡å­—ç¬¦åˆ‡åˆ†
from pnlp import cut_zhchar
text = "ä½ å¥½ï¼Œhello, 520 i love u. = â€æˆ‘çˆ±ä½ â€œã€‚"
char_list = cut_zhchar(text)
print(char_list)
"""
['ä½ ', 'å¥½', 'ï¼Œ', 'hello', ',', ' ', '520', ' ', 'i', ' ', 'love', ' ', 'u', '.', ' ', '=', ' ', 'â€', 'æˆ‘', 'çˆ±', 'ä½ ', 'â€œ', 'ã€‚']
"""
char_list = cut_zhchar(text, remove_blank=True)
print(char_list)
"""
['ä½ ', 'å¥½', 'ï¼Œ', 'hello', ',', '520', 'i', 'love', 'u', '.', '=', 'â€', 'æˆ‘', 'çˆ±', 'ä½ ', 'â€œ', 'ã€‚']
"""
```

#### å¥å­åˆ†ç»„

```python
from pnlp import combine_bucket
parts = [
    "å…ˆç”Ÿï¼Œé‚£å¤œï¼Œæˆ‘å› èƒ¸ä¸­çº³é—·ï¼Œæ— æ³•å…¥ç¡ï¼Œ",
    "æŠ˜è…¾å¾—æ¯”é‚£é“äº†è„šé•£çš„å›å˜æ°´æ‰‹è¿˜æ›´éš¾è¿‡ï¼›",
    "é‚£æ—¶ï¼Œæˆ‘å°±å†²åŠ¨çš„ â€”â€”",
    "å¥½åœ¨æœ‰é‚£ä¸€æ—¶ä¹‹å¿µï¼Œ",
    "å› ä¸ºæœ‰æ—¶æˆ‘ä»¬åœ¨æ— æ„ä¸­æ‰€åšçš„äº‹èƒ½å¤Ÿåœ†æ»¡â€¦â€¦"
]
buckets = combine_bucket(parts.copy(), 10, truncate=True, keep_remain=True)
print(buckets)
"""
['å…ˆç”Ÿï¼Œé‚£å¤œï¼Œæˆ‘å› èƒ¸ä¸­',
 'çº³é—·ï¼Œæ— æ³•å…¥ç¡ï¼Œ',
 'æŠ˜è…¾å¾—æ¯”é‚£é“äº†è„šé•£çš„',
 'å›å˜æ°´æ‰‹è¿˜æ›´éš¾è¿‡ï¼›',
 'é‚£æ—¶ï¼Œæˆ‘å°±å†²åŠ¨çš„ â€”',
 'â€”',
 'å¥½åœ¨æœ‰é‚£ä¸€æ—¶ä¹‹å¿µï¼Œ',
 'å› ä¸ºæœ‰æ—¶æˆ‘ä»¬åœ¨æ— æ„ä¸­',
 'æ‰€åšçš„äº‹èƒ½å¤Ÿåœ†æ»¡â€¦â€¦']
"""
```

### æ–‡æœ¬å¢å¼º

é‡‡æ ·å™¨æ”¯æŒåˆ é™¤ã€äº¤æ¢ã€æ’å…¥æ“ä½œï¼Œæ‰€æœ‰çš„æ“ä½œä¸ä¼šè·¨è¶Šæ ‡ç‚¹ã€‚

#### Tokençº§åˆ«

- é»˜è®¤ Tokenizer
    - ä¸­æ–‡ï¼šå­—ç¬¦çº§ Tokenizerï¼ˆè§ä¸Šï¼‰
    - è‹±æ–‡ï¼šç©ºç™½ç¬¦åˆ‡åˆ† Tokenizer
- Tokenizer å¯ä»¥ä»»æ„æŒ‡å®šï¼Œä½†å®ƒçš„è¾“å‡ºåº”è¯¥æ˜¯ä¸€ä¸ª List çš„ Token æˆ–ä¸€ä¸ª List çš„ Tupleï¼Œæ¯ä¸ª Tuple åŒ…å«ä¸€ä¸ª Token å’Œä¸€ä¸ªè¯æ€§ã€‚
- å¯¹å­—ç¬¦çº§å¢å¼ºï¼Œé»˜è®¤å¹¶ä¸ä¼šæ“ä½œæ‰€æœ‰å­—æˆ–è¯ã€‚å¯ä»¥è‡ªå®šä¹‰è¦æ“ä½œçš„è¯æˆ–è¯æ€§ã€‚
    - é»˜è®¤ Token æ˜¯ã€Œåœç”¨è¯ã€
    - é»˜è®¤è¯æ€§ï¼ˆå½“ Tokenizer è¾“å‡ºå¸¦è¯æ€§æ—¶ï¼‰æ˜¯ã€ŒåŠŸèƒ½è¯ã€ï¼šå‰¯è¯ã€ä»‹è¯ã€è¿è¯ã€åŠ©è¯ã€å…¶ä»–è™šè¯ï¼ˆæ ‡è®°ä¸º d p c u xcï¼‰

```python
# ã€ã€‘å†…çš„ä¸ºæ”¹å˜çš„
text = "äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚"
# å­—ç¬¦ç²’åº¦
from pnlp import TokenLevelSampler
tls = TokenLevelSampler()
tls.make_samples(text)
"""
{'delete': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦ã€æœ‰ã€‘æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'swap': 'ã€ä¸ºã€‘ã€äººã€‘ä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'insert': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼ã€è¿˜ã€‘è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'together': 'äººä»€ä¹ˆç€ç€æ´»ï¼Ÿç”Ÿè€Œå¿…ä¸ºä¸ºé¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚'}
"""
# æ”¯æŒè‡ªå®šä¹‰ tokenizer
tls.make_samples(text, jieba.lcut)
"""
{'delete': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººã€å¿…é¡»ã€‘è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'swap': 'ã€ä¸ºä»€ä¹ˆã€‘ã€äººã€‘æ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'insert': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼ã€è¿˜è¦ã€‘è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'together': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººäººè¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å¤šå°½å¯èƒ½çš„ç²¾ç¥ä½“éªŒã€‚'}
"""
# è‡ªå®šä¹‰
tls = TokenLevelSampler(
    rate=æ›¿æ¢æ¯”ä¾‹, # é»˜è®¤ 5%
    types=["delete", "swap", "insert"], # é»˜è®¤ä¸‰ä¸ª 
    sample_words=["è¯1", "è¯2"], # é»˜è®¤åœç”¨è¯
    sample_pos=["è¯æ€§1", "è¯æ€§2"], # é»˜è®¤åŠŸèƒ½è¯
)
```

#### å¥å­çº§åˆ«

```python
from pnlp import SentenceLevelSampler
sls = SentenceLevelSampler()
sls.make_samples(text)
"""
{'delete': 'ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'swap': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿè¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼',
 'insert': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿè¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼',
 'together': 'ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿäººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿ'}
"""
# è‡ªå®šä¹‰
sls = SentenceLevelSampler(types=["delete", "swap", "insert"]) # é»˜è®¤ä¸‰ä¸ª
```

### æ–‡æœ¬å½’ä¸€åŒ–

#### ä¸­æ–‡æ•°å­—

```python
from pnlp import num_norm
num_norm.num2zh(1024) == "ä¸€åƒé›¶äºŒåå››"
num_norm.num2zh(1024).to_money() == "å£¹ä»Ÿé›¶è´°æ‹¾è‚†"
num_norm.zh2num("ä¸€åƒé›¶äºŒåå››") == 1024
```

### æ ¼å¼è½¬æ¢

#### BIOè½¬å®ä½“

```python
# å®ä½“ BIO Token è½¬å®ä½“
from pnlp import pick_entity_from_bio_labels
pairs = [('å¤©', 'B-LOC'), ('å®‰', 'I-LOC'), ('é—¨', 'I-LOC'), ('æœ‰', 'O'), ('æ¯›', 'B-PER'), ('ä¸»', 'I-PER'), ('å¸­', 'I-PER')]
pick_entity_from_bio_labels(pairs)
"""
[('å¤©å®‰é—¨', 'LOC'), ('æ¯›ä¸»å¸­', 'PER')]
"""
pick_entity_from_bio_labels(pairs, with_offset=True)
"""
[('å¤©å®‰é—¨', 'LOC', 0, 3), ('æ¯›ä¸»å¸­', 'PER', 4, 7)]
"""
```

#### ä»»æ„å‚æ•°è½¬UUID

```python
from pnlp import generate_uuid

uid1 = pnlp.generate_uuid("a", 1, 0.02)
uid2 = pnlp.generete_uuid("a", 1)
"""
uid1 == 3fbc8b70d05b5abdb5badca1d26e1dbd
uid2 == f7b0ffc589e453e88d4faf66eb92f669
"""
```

### å†…ç½®è¯å…¸

#### åœç”¨è¯

```python
from pnlp import StopWords, chinese_stopwords, english_stopwords

csw = StopWords("/path/to/custom/stopwords.txt")
csw.stopwords # a set of the custom stopwords

csw.zh == chinese_stopwords # Chineses stopwords
csw.en == english_stopwords # English stopwords
```


### æ–‡æœ¬é•¿åº¦

```python
from pnlp import Length

text = "è¿™æ˜¯https://www.yam.gifté•¿åº¦æµ‹è¯•ï¼Œã€Š ã€‹*)FSJfdsjfğŸ˜![](http://xx.jpg)ã€‚233."

pl = Length(text)
# æ³¨æ„ï¼šå³ä½¿ä½¿ç”¨äº† patternï¼Œé•¿åº¦éƒ½æ˜¯åŸºäºåŸå§‹æ–‡æœ¬
# é•¿åº¦åŸºäºå­—ç¬¦è®¡æ•°ï¼ˆä¸æ˜¯æ•´è¯ï¼‰
print("Length of all characters: ", pl.len_all)
print("Length of all non-white characters: ", pl.len_nwh)
print("Length of all Chinese characters: ", pl.len_chi)
print("Length of all words and numbers: ", pl.len_wnb)
print("Length of all punctuations: ", pl.len_pun)
print("Length of all English characters: ", pl.len_eng)
print("Length of all numbers: ", pl.len_num)

"""
Length of all characters:  64
Length of all non-white characters:  63
Length of all Chinese characters:  6
Length of all words and numbers:  41
Length of all punctuations:  14
Length of all English characters:  32
Length of all numbers:  3
"""
```

### é­”æœ¯æ–¹æ³•

```python
from pnlp import MagicDict

# åµŒå¥—å­—å…¸
pmd = MagicDict()
pmd['a']['b']['c'] = 2
print(pmd)

"""
{'a': {'b': {'c': 2}}}
"""

# å½“å­—å…¸è¢«ç¿»è½¬æ—¶ï¼Œä¿ç•™æ‰€æœ‰çš„é‡å¤ value-keys
dx = {1: 'a',
      2: 'a',
      3: 'a',
      4: 'b' }
print(pmag.MagicDict.reverse(dx))

"""
{'a': [1, 2, 3], 'b': 4}
"""
```

### å¹¶è¡Œå¤„ç†

æ”¯æŒå››ç§å¹¶è¡Œå¤„ç†æ–¹å¼ï¼š

- çº¿ç¨‹æ± ï¼š`thread_pool`
- è¿›ç¨‹æ± ï¼š`process_pool`
- çº¿ç¨‹ Executorï¼š`thread_executor`ï¼Œé»˜è®¤ä½¿ç”¨
- çº¿ç¨‹ï¼š`thread`

æ³¨æ„ï¼šæƒ°æ€§å¤„ç†ï¼Œè¿”å›çš„æ˜¯ Generatorã€‚

```python
import math
def is_prime(x):
    if x < 2:
        return False
    for i in range(2, int(math.sqrt(x)) + 1):
        if x % i == 0:
            return False
    return True

from pnlp import concurring

# max_workers é»˜è®¤ä¸º 4
@concurring
def get_primes(lst):
    res = []
    for i in lst:
        if is_prime(i):
            res.append(i)
    return res

@concurrint(type="thread_pool", max_workers=10)
def get_primes(lst):
    pass
```

`concurring` è£…é¥°å™¨è®©ä½ çš„è¿­ä»£å‡½æ•°å¹¶è¡Œã€‚

## æµ‹è¯•

Clone ä»“åº“åæ‰§è¡Œï¼š

```bash
$ python -m pytest
```

## æ›´æ–°æ—¥å¿—

è§è‹±æ–‡ç‰ˆ READMEã€‚



