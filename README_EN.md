<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [Features](#features)
- [Install](#install)
- [Usage](#usage)
  - [Iopipe](#iopipe)
    - [IO process](#io-process)
    - [Built-in Method](#built-in-method)
  - [Text](#text)
    - [Clean and Extract](#clean-and-extract)
    - [Regex](#regex)
  - [Cut](#cut)
    - [AnypartCut](#anypartcut)
    - [SentenceCut](#sentencecut)
    - [ChineseCharCut](#chinesecharcut)
    - [CombineBucket](#combinebucket)
  - [Enhancement](#enhancement)
  - [Normalization](#normalization)
  - [Transformation](#transformation)
  - [StopWords](#stopwords)
  - [Length](#length)
  - [Magic](#magic)
  - [Concurring](#concurring)
- [Test](#test)
- [ChangeLog](#changelog)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

This is a pre-processing tool for NLP.

## Features

- A flexible pipe line for text io
- A flexible tool for text clean and extract
- Text enhancement
- Sentence cut and Chinese character cut
- Text bucket
- Chinese character normalization
- Kinds of length
- Stopwords
- Some magic usage in pre-processing
- Tools like Concurring, generating batches

## Install

Need Python3.7+.

`pip install pnlp`

## Usage

### Iopipe

#### IO process

```bash
tree tests/piop_data/
â”œâ”€â”€ a.md
â”œâ”€â”€ b.txt
â”œâ”€â”€ c.data
â”œâ”€â”€ first
â”‚Â Â  â”œâ”€â”€ fa.md
â”‚Â Â  â”œâ”€â”€ fb.txt
â”‚Â Â  â”œâ”€â”€ fc.data
â”‚Â Â  â””â”€â”€ second
â”‚Â Â      â”œâ”€â”€ sa.md
â”‚Â Â      â”œâ”€â”€ sb.txt
â”‚Â Â      â””â”€â”€ sc.data
â”œâ”€â”€ json.json
â”œâ”€â”€ outfile.file
â”œâ”€â”€ outjson.json
â””â”€â”€ yml.yml
```

```python
import os
from pnlp import Reader

DATA_PATH = "./pnlp/tests/piop_data/"
pattern = '*.md' # also could be '*.txt', 'f*.*', etc. SUPPORT regex
reader = Reader(pattern, use_regex=True)

# Get lines of all files in one directory with line index and file name
for line in reader(DATA_FOLDER_PATH):
    print(line.lid, line.fname, line.text)
"""
0 a.md line 1 in a.
1 a.md line 2 in a.
2 a.md line 3 in a.
0 fa.md line 1 in fa.
1 fa.md line 2 in fa
...
"""

# Get lines of one file lines with line index and file name
# if a file is read, the `pattern` is not effective
for line in reader(os.path.join(DATA_FOLDER_PATH, "a.md")):
    print(line.lid, line.fname, line.text)
"""
0 a.md line 1 in a.
1 a.md line 2 in a.
2 a.md line 3 in a.
"""



# Get all filepaths in one directory
for path in Reader.gen_files(DATA_PATH, pattern):
    print(path)
"""
pnlp/tests/piop_data/a.md
pnlp/tests/piop_data/first/fa.md
pnlp/tests/piop_data/first/second/sa.md
"""

# Get content(article) of all files in one directory with file name
paths = Reader.gen_files(DATA_PATH, pattern)
articles = reader.gen_articles(paths)
for article in articles:
    print(article.fname)
    print(article.f.read())
"""
a.md
line 1 in a.
line 2 in a.
line 3 in a.
...
"""

# Get lines of all files in one directory with line index and file name
# the same as ip.Reader(DATA_PATH, pattern)
paths = Reader.gen_files(DATA_PATH, pattern)
articles = Reader.gen_articles(paths)
for line in Reader.gen_flines(articles):
    print(line.lid, line.fname, line.text)
```

#### Built-in Method

```python
import pnlp

# Read
file_string = pnlp.read_file(file_path)
file_list = pnlp.read_lines(file_path)
file_json = pnlp.read_json(file_path)
file_yaml = pnlp.read_yaml(file_path)
file_csv = pnlp.read_csv(file_path)

# Write
pnlp.write_json(file_path, data, indent=2)
pnlp.write_file(file_path, data)

# Others
pnlp.check_dir(dirname) # will make dirname if not exist
```

### Text

#### Clean and Extract

```python
import re

# Use Text
from pnlp import Text

text = "è¿™æ˜¯https://www.yam.gifté•¿åº¦æµ‹è¯•ï¼Œã€Š ã€‹*)FSJfdsjfğŸ˜![](http://xx.jpg)ã€‚233."
pattern = re.compile(r'\d+')

# pattern is re.Pattern or str type
# Default is '', means do not use any pattern (acctually is re.compile(r'.+'). In this pattern, clean returns nothing, extract returns the origin.
# If pattern is a string, a build-in pattern will be used, there are 11 types:
#	'chi': Chinese character
#	'pun': Punctuations
#	'whi': White space
#	'nwh': Non White space
#	'wnb': Word and number
#	'nwn': Non word and number
#	'eng': English character
#	'num': Number
#	'pic': Pictures
#	'lnk': Links
#	'emj': Emojis

pt = Text(['chi', pattern])
# pt.extract will return matches and their locations
res = pt.extract(text)
print(res)
"""
{'text': 'è¿™æ˜¯é•¿åº¦æµ‹è¯•233', 'mats': ['è¿™æ˜¯', 'é•¿åº¦æµ‹è¯•', '233'], 'locs': [(0, 2), (22, 26), (60, 63)]}
"""
# support use dot to get the key field
print(res.text, res.mats, res.locs)
"""
'è¿™æ˜¯é•¿åº¦æµ‹è¯•' ['è¿™æ˜¯', 'é•¿åº¦æµ‹è¯•'] [(0, 2), (22, 26)]
"""
# pt.clean will return cleaned text using the pattern
print(pt.clean(text))
"""
https://www.yam.giftï¼Œã€Š ã€‹*)FSJfdsjfğŸ˜![](http://xx.jpg)ã€‚233.
"""

pt = Text(['pic', 'lnk'])
res = pt.extract(text)
print(res.mats)
"""
['https://www.yam.gif',
 '![](http://xx.jpg)',
 'https://www.yam.gift',
 'http://xx.jpg']
"""
print(pt.clean(text))
"""
è¿™æ˜¯té•¿åº¦æµ‹è¯•ï¼Œã€Š ã€‹*)FSJfdsjfğŸ˜ã€‚233.
"""
```

#### Regex

```python
# USE Regex
from pnlp import reg
def clean_text(text: str) -> str:
    text = reg.pwhi.sub("", text)
    text = reg.pemj.sub("", text)
    text = reg.ppic.sub("", text)
    text = reg.plnk.sub("", text)
    return text
```

### Cut

#### AnypartCut

```python
# Cut by Regex
from pnlp import cut_part, psent
text = "ä½ å¥½ï¼æ¬¢è¿ä½¿ç”¨ã€‚"
sent_list = cut_part(text, psent, with_spliter=True, with_offset=False)
print(sent_list)
"""
['ä½ å¥½ï¼', 'æ¬¢è¿ä½¿ç”¨ã€‚']
"""
pcustom_sent = re.compile(r'[ã€‚ï¼]')
sent_list = cut_part(text, pcustom_sent, with_spliter=False, with_offset=False)
print(sent_list)
"""
['ä½ å¥½', 'æ¬¢è¿ä½¿ç”¨']
"""
sent_list = cut_part(text, pcustom_sent, with_spliter=False, with_offset=True)
print(sent_list)
"""
[('ä½ å¥½', 0, 3), ('æ¬¢è¿ä½¿ç”¨', 3, 8)]
"""
```

#### SentenceCut

```python
# Cut Sentence
from pnlp import cut_sentence as pcs
text = "ä½ å¥½ï¼æ¬¢è¿ä½¿ç”¨ã€‚"
sent_list = pcs(text)
print(sent_list)
"""
['ä½ å¥½ï¼', 'æ¬¢è¿ä½¿ç”¨ã€‚']
"""
```

#### ChineseCharCut

```python
# Cut to Chinese chars
from pnlp import cut_zhchar
text = "ä½ å¥½ï¼Œhello, 520 i love u. = â€æˆ‘çˆ±ä½ â€œã€‚"
char_list = cut_zhchar(text)
print(char_list)
"""
['ä½ ', 'å¥½', 'ï¼Œ', 'hello', ',', ' ', '520', ' ', 'i', ' ', 'love', ' ', 'u', '.', ' ', '=', ' ', 'â€', 'æˆ‘', 'çˆ±', 'ä½ ', 'â€œ', 'ã€‚']
"""
char_list = cut_zhchar(text, remove_blank=True)
print(char_list)
"""
['ä½ ', 'å¥½', 'ï¼Œ', 'hello', ',', '520', 'i', 'love', 'u', '.', '=', 'â€', 'æˆ‘', 'çˆ±', 'ä½ ', 'â€œ', 'ã€‚']
"""
```

#### CombineBucket

```python
from pnlp import combine_bucket
parts = [
    "å…ˆç”Ÿï¼Œé‚£å¤œï¼Œæˆ‘å› èƒ¸ä¸­çº³é—·ï¼Œæ— æ³•å…¥ç¡ï¼Œ",
    "æŠ˜è…¾å¾—æ¯”é‚£é“äº†è„šé•£çš„å›å˜æ°´æ‰‹è¿˜æ›´éš¾è¿‡ï¼›",
    "é‚£æ—¶ï¼Œæˆ‘å°±å†²åŠ¨çš„ â€”â€”",
    "å¥½åœ¨æœ‰é‚£ä¸€æ—¶ä¹‹å¿µï¼Œ",
    "å› ä¸ºæœ‰æ—¶æˆ‘ä»¬åœ¨æ— æ„ä¸­æ‰€åšçš„äº‹èƒ½å¤Ÿåœ†æ»¡â€¦â€¦"
]
buckets = combine_bucket(parts.copy(), 10, truncate=True, keep_remain=True)
print(buckets)
"""
['å…ˆç”Ÿï¼Œé‚£å¤œï¼Œæˆ‘å› èƒ¸ä¸­',
 'çº³é—·ï¼Œæ— æ³•å…¥ç¡ï¼Œ',
 'æŠ˜è…¾å¾—æ¯”é‚£é“äº†è„šé•£çš„',
 'å›å˜æ°´æ‰‹è¿˜æ›´éš¾è¿‡ï¼›',
 'é‚£æ—¶ï¼Œæˆ‘å°±å†²åŠ¨çš„ â€”',
 'â€”',
 'å¥½åœ¨æœ‰é‚£ä¸€æ—¶ä¹‹å¿µï¼Œ',
 'å› ä¸ºæœ‰æ—¶æˆ‘ä»¬åœ¨æ— æ„ä¸­',
 'æ‰€åšçš„äº‹èƒ½å¤Ÿåœ†æ»¡â€¦â€¦']
"""
```

### Enhancement

Sampler support delete, swap and insert operation, all operations do not span punctuations.

#### TokenLevel

- It uses a default tokenizer for Chinese (Chinese Char Tokenizer) and English (Simple Whitespace Tokenizer).
- The tokenizer could be anyone you like, but the output should be a list of tokens or a list of tuple pairs, each pair include a token and a part-of-speech.
- It uses `stopwords` as default sample words and function part-of-speech as default sample pos. This means we only sampling those tokens who are in the sample words or their pos are in the sample pos (if they just have a pos). You could customize them as you like.

```python
# tokens in ã€ã€‘ are operated
text = "äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚"
# TokenLevel
from pnlp import TokenLevelSampler
tls = TokenLevelSampler()
tls.make_samples(text)
"""
{'delete': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦ã€æœ‰ã€‘æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'swap': 'ã€ä¸ºã€‘ã€äººã€‘ä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'insert': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼ã€è¿˜ã€‘è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'together': 'äººä»€ä¹ˆç€ç€æ´»ï¼Ÿç”Ÿè€Œå¿…ä¸ºä¸ºé¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚'}
"""
# tokenizer is supported
tls.make_samples(text, jieba.lcut)
"""
{'delete': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººã€å¿…é¡»ã€‘è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'swap': 'ã€ä¸ºä»€ä¹ˆã€‘ã€äººã€‘æ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'insert': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼ã€è¿˜è¦ã€‘è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'together': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿç”Ÿè€Œä¸ºäººäººè¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å¤šå°½å¯èƒ½çš„ç²¾ç¥ä½“éªŒã€‚'}
"""
# custom the Sampler
tls = TokenLevelSampler(
    rate=replace_rate, # default 5%
    types=["delete", "swap", "insert"], # default the three
    sample_words=["w1", "w2"], # default is the stopwords
    sample_pos=["pos1", "pos2"], # default is the functional pos (d p c u xc), means adv, prep, conj, auxiliary,  other functional pos
)
```

#### SentenceLevel

```python
from pnlp import SentenceLevelSampler
sls = SentenceLevelSampler()
sls.make_samples(text)
"""
{'delete': 'ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼è¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚',
 'swap': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿè¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼',
 'insert': 'äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿè¿˜è¦æœ‰å°½å¯èƒ½å¤šçš„ç²¾ç¥ä½“éªŒã€‚ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼',
 'together': 'ç”Ÿè€Œä¸ºäººå¿…é¡»è¦æœ‰æ¢¦æƒ³ï¼äººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿäººä¸ºä»€ä¹ˆæ´»ç€ï¼Ÿ'}
"""
# custom the Sampler
sls = SentenceLevelSampler(types=["delete", "swap", "insert"]) # default is the three
```

### Normalization

#### ChineseNumber

```python
from pnlp import num_norm
num_norm.num2zh(1024) == "ä¸€åƒé›¶äºŒåå››"
num_norm.num2zh(1024).to_money() == "å£¹ä»Ÿé›¶è´°æ‹¾è‚†"
num_norm.zh2num("ä¸€åƒé›¶äºŒåå››") == 1024
```

### Transformation

#### BIO2Entity

```python
# entity bio to entities
from pnlp import pick_entity_from_bio_labels
pairs = [('å¤©', 'B-LOC'), ('å®‰', 'I-LOC'), ('é—¨', 'I-LOC'), ('æœ‰', 'O'), ('æ¯›', 'B-PER'), ('ä¸»', 'I-PER'), ('å¸­', 'I-PER')]
pick_entity_from_bio_labels(pairs)
"""
[('å¤©å®‰é—¨', 'LOC'), ('æ¯›ä¸»å¸­', 'PER')]
"""
pick_entity_from_bio_labels(pairs, with_offset=True)
"""
[('å¤©å®‰é—¨', 'LOC', 0, 3), ('æ¯›ä¸»å¸­', 'PER', 4, 7)]
"""
```

### Built-in Dicts

#### StopWords

```python
from pnlp import StopWords, chinese_stopwords, english_stopwords

csw = StopWords("/path/to/custom/stopwords.txt")
csw.stopwords # a set of the custom stopwords

csw.zh == chinese_stopwords # Chineses stopwords
csw.en == english_stopwords # English stopwords
```


### Length

```python
from pnlp import Length

text = "è¿™æ˜¯https://www.yam.gifté•¿åº¦æµ‹è¯•ï¼Œã€Š ã€‹*)FSJfdsjfğŸ˜![](http://xx.jpg)ã€‚233."

pl = Length(text)
# Note that even a pattern is used, the length is always for the raw text.
# Length is counted by character, not entire word or number.
print("Length of all characters: ", pl.len_all)
print("Length of all non-white characters: ", pl.len_nwh)
print("Length of all Chinese characters: ", pl.len_chi)
print("Length of all words and numbers: ", pl.len_wnb)
print("Length of all punctuations: ", pl.len_pun)
print("Length of all English characters: ", pl.len_eng)
print("Length of all numbers: ", pl.len_num)

"""
Length of all characters:  64
Length of all non-white characters:  63
Length of all Chinese characters:  6
Length of all words and numbers:  41
Length of all punctuations:  14
Length of all English characters:  32
Length of all numbers:  3
"""
```

### Magic

```python
from pnlp import MagicDict

# Nest dict
pmd = MagicDict()
pmd['a']['b']['c'] = 2
print(pmd)

"""
{'a': {'b': {'c': 2}}}
"""

# Preserve all repeated value-keys when a Dict is reversed.
dx = {1: 'a',
      2: 'a',
      3: 'a',
      4: 'b' }
print(pmag.MagicDict.reverse(dx))

"""
{'a': [1, 2, 3], 'b': 4}
"""
```

### Concurring

Support 4 types of concurring:

- `thread_pool`
- `process_pool`
- `thread_executor`, the default
- `thread`

Note that we use lazy process, return  generators.

```python
import math
def is_prime(x):
    if x < 2:
        return False
    for i in range(2, int(math.sqrt(x)) + 1):
        if x % i == 0:
            return False
    return True

from pnlp import concurring

# the default value of `max_workers` is 4
@concurring
def get_primes(lst):
    res = []
    for i in lst:
        if is_prime(i):
            res.append(i)
    return res

@concurrint(type="thread", max_workers=10)
def get_primes(lst):
    pass
```

`concurring` wrapper just make your original function concurring. 

## Test

Clone the repo run:

```bash
$ python -m pytest
```

## ChangeLog


**v0.4.6**

Fix regex: `-` if is used as string literal, should be transfered.

**v0.4.5**

Add loc to bio label => entity.

**v0.4.3**

Adjust `Reader` init parameters.

**v0.4.2**

Add bio label => entity.

**v0.4.1**

Remove annotation `re.Pattern`.

**v0.4.0**

Make dataclass their right usage.


**v0.3.11**

Adjust `MagicDict` and `check_dir`.

**v0.3.10**

Fix piop `strip`.

**v0.3.9**

`Reader` support regex.

**v0.3.8**

Fix `concurring` for multiple processing.

**v0.3.7**

Add concurring and batch generator

**v0.3.5**

Add text enhancement.

**v0.3.3/4**

Fix url link and picture  `Regex` pattern.

**v0.3.2**

Fix `cut_part` for sentence ends with a white space and a full stop. 

**v0.3.1**

Add `cut_part` to cut text to any parts by the given Regex Pattern; Add `combine_bucket` to combine any parts to buckets by the given threshold(length).

**v0.3.0**

Update `cut_sentence`; Add `NumNorm`.

**v0.28-29**

Update `cut_zhchar`.

**v0.27**

Add `cut_zhchar`.

**v0.26**

Add `read_csv`, remove `ï¼›` as a sentence cut standard.

**v0.25**

Add `stop_words`. 

**v0.24**

Fix `read_json`.

**v0.23**

Fix `Text` default rule.

**v0.22**

Make `Text` more convenient to use.

**v0.21**

Add `cut_sentence` method.

**v0.20**

Optimize several interface and make `Text` accept list of Regular Expression Patterns.

